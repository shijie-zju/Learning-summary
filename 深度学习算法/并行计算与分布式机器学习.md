并行计算与深度学习1

并行计算

1. mapreduce:信息通信，workerserver,同步

server一次分发g同步后再更新w

2. parameter server:信息通信，workerserver,异步

server接收某w就更新g，不同于随机梯度

3. decentralized:信息通信，peertopeer去中心化,异步

某节点平均周边w后自己内部更新w


机器学习与并行计算的关系，机器学习由于技术的发展，大模型、大数据的影响致使模型训练只在一个GPU上运行不显示，需要多个GPU来加速，减少钟表时间。

1、线性回归与最小二乘

线性回归就是用数据特征的加权平均来估算预测目标。


损失函数，希望预测值与实际值越接近越好，即，两者差的平方也是越小越好。 


接着就可以建立优化问题【最小二乘回归】，找到一个W使得这个损失最小，L(W)是目标函数，W是优化变量，问题的最优解记为W*

2、如何使用并行梯度下降来解最小二乘

首先对损失函数求关于变量w的一阶导数，即梯度

梯度的物理意义：在一个点w上沿着梯度方向走很小一步，那么函数值L(W)一定时上升的，如果沿着梯度相反的方向(即前面乘个负号)找到下降方向。由于我们希望损失函数值越小越好，因此需要沿着梯度相反的方向走。在下面的图中，α叫做步长也称为学习率。


由于每一轮都需要计算梯度，并且与样本量和W的维度也有关。因此计算梯度也是算法的瓶颈了，如果能够把梯度计算并行化，那么就能计算得更快。

如何用两个处理器加速计算？

通信是并行计算的关键


3、实现并行梯度下降的一些算法

MapReduce



并行计算的加速比


通信代价：通信复杂度（与模型参数量、节点个数成反比）；网络延迟


通信代价：通信复杂度（与模型参数量、节点个数成反比）；网络延迟；同步操作

MapReduce总结：算法与普通单机上的训练轮次一样，同步保证了收敛性；算法时间成本主要有计算、通信、同步这三部分组成；加速比小于m


并行计算与深度学习2

异步梯度下降

在理论收敛上，异步算法比同步的要慢。但在实际上，异步避免了同步造成的时间浪费。

实现异步梯度下降的编程模型：

1、parameter server architecture


该算法是由以下人员提出，建议使用Ray框架


比较一下同步和异步的工作时间

同步：


异步：


 异步梯度下降的代码编写

worker端的计算：向server发送请求最新的模型参数W（一次通信），然后计算本地梯度，接着向server发送本地梯度（又一次通信）。

server端:监听worker端的请求；接收worker端传来的梯度并更新


异步的优缺点：如果一个worke非常慢，收敛就会收影响。


受影响原因：因为慢的那个提供的梯度落后了好几轮，没什么信息了，是指使用该梯度信息还会使整体的参数变差


因此，异步算法的实现是有限制的。联邦学习就存在这些问题。

2、decentralized network去中心化网络


每个节点的训练过程：使用本地数据计算本地梯度，向领接节点要参数，接着把自己的参数与连接节点的参数做加权平均得到新参数，最后节点本地做一次梯度下降，更新自己的参数。重复上述步骤。


去中心化和随机梯度下降都是可以收敛的。去中心化的收敛性与节点网络构成的图的紧密程度有关，越紧密越快，如果这个图可以拆成两个或多个部分，那么算法根本就不会收敛。


总结：



并行计算与分布式计算（两者差别不大，两者概念都在混用）：


并行计算与深度学习3

使用TensorFlow实现并行运算：。。。

Rey和reduce的技术细节：。。。

这一块之后再学习。



联邦学习

前言：从技术以及研究方向来讲解联邦学习

联邦学习的例子：1、移动端用户数据的采集与应用2、医院、银行、保险公司的数据共享。


联邦学习本质上是分布式学习，但还是有些区别的，联邦学习：

用户对自己的数据有着很大的控制权
用户端不稳定
通信代价远大于计算开销
数据非独立同分布
用户节点数据量不平衡，即负载不均衡

联邦学习研究方向:

1、通信效率


federated averaging algorithm

客户端：


服务器：





epochs:所有用户把自己本地数据计算一遍就算是一个epoch。因此epoc数量的多少就可以来衡量计算量的多少。


文章3证明了fedavg算法在Non-IID数据下的收敛性


2、隐私保护


但是梯度包含了用户信息，它只是把数据变换了一下，梯度携带了用户隐私



加噪声不太行，保护隐私有挑战

3、鲁棒性

﻿
4. 联邦学习：技术角度的讲解（中文）Introduction t... P4 - 28:42
﻿

是联邦学习可以抵御拜占庭错误和恶意攻击

拜占庭错误：节点中有使坏的节点


下毒攻击，干扰学习，留后门

防御方法，但这些方法大多假设数据独立同分布：

