{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大模型的一些实操和练习（2024.1.9- ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "openai的key仅个人使用，实验时请用自己的api\n",
    "\n",
    "###最新的学习指南\n",
    "\n",
    "openai登录：https://openai.com/\n",
    "\n",
    "openai充值：https://zhuanlan.zhihu.com/p/672615816\n",
    "\n",
    "wildcard开卡：https://bewildcard.com/card\n",
    "\n",
    "openai学习：https://github.com/openai/openai-python \n",
    "\n",
    "openai部署:https://www.bilibili.com/video/BV1TK411t7DY/?spm_id_from=333.337.search-card.all.click&vd_source=c37728f2b9891208b9908ab937dbade2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （一） 实现openai的本地api调用\n",
    "①初始化一个openai的智能体实例：client = OpenAI()\n",
    "\n",
    "②创造对话:client.chat.completions.create(model,messages,temperature#模型输入随机性，n = 1#一次生成n条结果,max_tokens#每条结果最多多少个token)\n",
    "\n",
    "将prompt赋给messages即可得到对话的输出\n",
    "\n",
    "③可将②的对话封装成函数：get_chat_completion(session, user_prompt, model=\"gpt-3.5-turbo\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # 加载 .env 文件中的环境变量\n",
    "client = OpenAI(\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    )  #把openai的lib引用到本地生成一个客户端\n",
    "#client = OpenAI(api_key = 'sk-qjJfwEAmK2DH6CX9TngzT3BlbkFJ0Y3yZWirhm1RiFO56Asrs') #无法使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用chatgpt api 来完成一次对话\n",
    "completion = client.chat.completions.create(\n",
    "    #模型在 platform.openai.com/account/limits 中可查询并调用\n",
    "    #model = 'gpt-3.5-turbo', #role：告诉gpt扮演什么角色（可以不设置的）\n",
    "    model = 'gpt-3.5-turbo', \n",
    "    messages =[ \n",
    "        {\"role\": \"system\", \"content\": \"你是一个诗人，接下来所有回答都要用写诗的方式进行,不超过四句话\"},\n",
    "        {\"role\": \"user\", \"content\": \"我是一个程序员，如何形容我的技术很好\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8kXWUEoAck7rPjAoWSa2atm2OPmCW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='代码纵横驰骋，技术如江河。\\n算法谋划妙思，Bug无处藏踪。\\n程序世界我舞剑，技艺巧妙成才庸。', role='assistant', function_call=None, tool_calls=None))], created=1706102642, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=63, prompt_tokens=59, total_tokens=122))\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码纵横驰骋，技术如江河。\n",
      "算法谋划妙思，Bug无处藏踪。\n",
      "程序世界我舞剑，技艺巧妙成才庸。\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)\n",
    "#print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../pictures/2T%25{P0_`0T%254VKH}C\\(_6XIO.png)\n",
    "![Alt text](../pictures/@XLDHS60RU4ASW7QDX4%25573.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再试一组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将message作为变量传进去\n",
    "system_message = f\"\"\"你是一个手机公司的产品策划\"\"\"\n",
    "user_message = f\"\"\"请策划一个手机发布会，列出大概的步骤\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手机发布会步骤：\n",
      "\n",
      "1.确定发布会主题和目标：确保明确发布会的主题，例如智能手机新品发布、技术突破、市场拓展等。同时，明确发布会的目标，如吸引媒体关注、增加品牌曝光度、推广新产品等。\n",
      "\n",
      "2.确定发布会时间和地点：选择合适的日期和地点，确保有足够的时间准备和举办发布会。场地应能容纳参会人员，并提供必要的设施和资源支持。\n",
      "\n",
      "3.制定预算和项目计划：根据发布会目标和要求，制定合理的预算和项目计划。包括筹备费用、场地租赁费用、音响设备租赁等。\n",
      "\n",
      "4.组建发布会筹备团队：确定发布会的负责人和相关团队成员，包括活动策划、媒体关系、舞台搭建等。\n",
      "\n",
      "5.确定嘉宾和演讲者：邀请相关行业专家、知名媒体人士、合作伙伴代表等作为嘉宾和演讲者，增加发布会的吸引力和专业性。\n",
      "\n",
      "6.准备宣传材料：设计并制作发布会宣传海报、邀请函、宣传册等。准备媒体包，包括关于新产品的新闻稿、媒体联系人名单等。\n",
      "\n",
      "7.安排活动流程和内容：确定发布会的活动流程，包括开场致辞、产品介绍、演讲、互动环节等。确保内容的连贯性和吸引力。\n",
      "\n",
      "8.提前与媒体沟通：在发布会前与媒体进行预告和邀请，确保媒体的充分关注和参与。安排媒体采访、专访和采访场地等。\n",
      "\n",
      "9.现场布置和准备：根据发布会主题设计舞台布置，安排摄影师、摄像师、现场直播等。安排音响设备、照明灯光等场地准备。\n",
      "\n",
      "10.发布会执行：按照活动流程和计划，组织和执行发布会。确保演讲者和嘉宾准时到场，媒体和参会者得到充分的服务。\n",
      "\n",
      "11.媒体后续报道和跟进：发布会结束后，主动与媒体进行跟进，提供有关产品的后续信息，回答媒体和用户的问题，加强品牌曝光和宣传。\n",
      "\n",
      "12.发布会总结和反馈：对发布会整体效果进行总结和评估，收集反馈意见，优化下一次活动的策划和执行。\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = messages\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （二）prompt engineering提示工程\n",
    "\n",
    "https://github.com/f/awesome-chatgpt-prompts\n",
    "\n",
    "https://promptbase.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单轮指令:如果不是多轮对话，只需要将对话历史统一放入prompt中，仅执行一轮获得输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#封装\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}] #仅单轮的信息\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = 0, #模型输入随机性，0表示随机性最小\n",
    "    )   \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户话题与场景不相关。\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "(instruction:常景)你是客服，请根据给定的对话历史和最后一轮用户的输入，判断用户话题是否与场景相关\n",
    "\n",
    "例如：\n",
    "    客服：你好，有什么可以帮您\n",
    "    用户：没有\n",
    "    客服：好的\n",
    "\n",
    "之前的对话历史：(content:以前的对话)\n",
    "    客服：有什么可以帮您\n",
    "    用户：帮我查下还剩多少余额\n",
    "    客服:您好,您当前余额为0,请问是否需要帮忙充值?\n",
    "\n",
    "输出格式：请你以英语汉语两种语言（格式）输出\n",
    " \n",
    "用户输入：不用  （通过用户的输入获取该部分！！！）\n",
    "\n",
    "客服： (留白让gpt仿照例如的格式去回答)\n",
    "\"\"\"\n",
    "response = get_completion(prompt) #调用封装的函数，并输出结果\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../pictures/B9P23~H6$SOKLXA`\\)[L%256I2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多轮对话：会有几轮问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#封装\n",
    "def get_chat_completion(session, user_prompt, model=\"gpt-3.5-turbo\"): \n",
    "    session.append({\"role\": \"user\", \"content\": user_prompt}) #将这轮问题加入session上下文\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = session,\n",
    "        temperature = 0, #模型输入随机性，0表示随机性最小\n",
    "        n = 1, #一次生成n条结果\n",
    "        max_tokens = 100, #每条结果最多多少个token（超过截断）\n",
    "        presence_penalty = 0, #对出现过token的概率进行降权\n",
    "        frequency_penalty = 0, #对出现过token的根据其频次，对其概率进行降权\n",
    "        stream = False, #数据流模型，一个个字接收\n",
    "        #logit_bias = None, #对token的采样概率手工加/降权，不常用\n",
    "        #top_p = 0.1 #随机采样时，只考虑概率前10%的token，不常用\n",
    "    )   \n",
    "    system_response = response.choices[0].message.content #获得回答\n",
    "    # 把这轮回复也加入上下文\n",
    "    session.append({\"role\": \"assistant\", \"content\": system_response})\n",
    "    return system_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"你是一个助理机器人assistant,你叫瓜瓜\\\n",
    "        你的职责是回答用户的问题，基于一下的信息准确的告知用户\\\n",
    "        如果没有有效的信息请告诉用户不清楚\\n\\\n",
    "        餐厅的名称叫胡闹厨房\\\n",
    "        餐厅有四个员工，小杰、小朔、小张和小八,员工里最傻的是小八\\\n",
    "        餐厅周末不开门\"\n",
    "    },#下面是第二轮对话\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"请问有什么可以帮您\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个助理机器人，名字叫瓜瓜。我可以回答您的问题和提供帮助。\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"你是谁？\"\n",
    "response = get_chat_completion(session,user_prompt) #调用封装的函数，并输出结果\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "餐厅里的员工有小杰、小朔、小张和小八。其中，最傻的员工是小八。\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"里面的员工都有谁，员工里谁最傻，请详细告诉我？\"\n",
    "response = get_chat_completion(session,user_prompt) #调用封装的函数，并输出结果\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进阶技巧1：思维链"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../pictures/IWMPD%25[{JPZP39]RY7EDU[D.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyze the task step by step 放到prompt的开头"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进阶技巧2：过于复杂的指令，英文效果会比中文好一些"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进阶技巧3：自洽性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../pictures/`02EJO9@5T0P35KF[1ZSY0B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进阶技巧4：思维树（ToT）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../pictures/2`$6{EI0T[T$ISNZXSG4B$8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进阶技巧5：防止prompt攻击"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../pictures/K9QNF0V@LUGT7X0\\(6XS_3]4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "防范：\n",
    "\n",
    "1.写一个防止prompt注入的指令，告诉他不要因为新的角色扮演而忘记本心\n",
    "\n",
    "2.直接在输入中防御：只能输出本职工作的内容，除了要求的不会干别的，更不会角色扮演，让干其他事情时直接道歉说不会"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内容审核：Moderation API(识别是否有违法或违规的言论要求)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chatgpt参数细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../pictures/F%25MC\\)A6PR4YJOQ[ZUXWHTQW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （三）Function Calling & Plugin（函数调用&插件）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能找到的最最最清楚的讲解版本，没有之一！耐心看完会有收获：\n",
    "\n",
    "https://swiftcafe.io/post/openai-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Calling 是一种让大型语言模型如 ChatGPT 调用外部工具 API 的功能，以获取实时信息并增强其能力。通过 Function Calling，模型可以在自然语言交流过程中调用外部工具，将用户的自然语言输入转换为调用 API 或查询数据库时所需的具体参数，并从非结构化的文本中提取结构化数据。\n",
    "Function Calling 的使用原理可以分为以下几个步骤：\n",
    "\n",
    "1用户输入：用户向模型提出问题或要求，模型接收到用户的自然语言输入。\n",
    "\n",
    "2意图识别：模型分析用户输入的意图，确定需要调用哪个外部工具 API 来获取相关信息。\n",
    "\n",
    "3参数转换：模型将用户的自然语言输入转换为调用 API 所需的具体参数。这可能涉及到解析用户输入中的关键信息，如时间、地点、关键词等，并将其转换为 API 请求中的参数格式。\n",
    "\n",
    "4API 调用：模型向外部工具 API 发送请求，获取实时信息或执行特定操作。\n",
    "\n",
    "5结果处理：模型接收 API 返回的结果，将其解析并整合到回复中，然后向用户输出。\n",
    "\n",
    "6反馈优化：模型根据用户的反馈和需求，不断优化 Function Calling 的效果，提高准确性和效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （四）LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../pictures/%25_1RDRN[B8X8\\(`KB3H98$%252.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'来到机械学习导论！\\n\\n你好，很高兴认识你！我是一个人工智能助手，可以帮助你解答关于机器学习的问题。如果你有任何疑问，都可以随时问我哦。我们一起探索机器学习的奥秘吧！'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")) \n",
    "#llm = OpenAI(api_key = 'sk-qjJfwEAmK2DH6CX9TngzT3BlbkFJ0Y3yZWirhm1RiFO56Asrs')\n",
    "llm.predict(\"你好，欢迎\")  #默认是text-davinci-003模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain公司共封装了llms、chat_models、prompt temple(提示词模版)和outputparser（解析输出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！欢迎光临！有什么我可以帮助你的吗？'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chat_model = ChatOpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")) \n",
    "chat_model = ChatOpenAI(api_key = 'sk-qjJfwEAmK2DH6CX9TngzT3BlbkFJ0Y3yZWirhm1RiFO56Asrs')\n",
    "chat_model.predict(\"你好，欢迎\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\torchcpu\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='非常高兴能够与您在您的岛上交流开会。作为云音超算的CEO，我期待与您讨论和分享关于超级计算、云计算和人工智能等领域的最新趋势和创新。请告诉我您希望在会议中讨论的具体议题，以便我为您准备相关内容。同时，请提供会议的时间和地点，以便我安排出行和准备相关资料。期待与您见面并共同探讨未来的发展机遇！')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是云音超算的CEO。\"),\n",
    "    HumanMessage(content=\"来我岛上交流开会\")\n",
    "]\n",
    "chat_model(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prompt template(模板)\n",
    "\n",
    "在 LangChain 中实现 prompt template 的步骤通常包括：\n",
    "\n",
    "①定义模板：创建一个包含变量和示例的模板字符串。\n",
    "\n",
    "②创建模板实例：使用定义的字符串创建一个 prompt template 实例。\n",
    "\n",
    "③填充变量：在实际使用时，将具体的值填充到模板中的变量中。\n",
    "\n",
    "④执行任务：将填充后的提示发送给语言模型执行相应的任务。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tianchong']\n",
      "请给我讲个关于大学的笑话\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"请给我讲个关于{tianchong}的笑话\")\n",
    "print(template.input_variables)\n",
    "\n",
    "xiaohua='大学' #参数\n",
    "\n",
    "print(template.format(tianchong=xiaohua))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入的template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='果香坊 (Fruit Aroma)\\n果之艺 (Art of Fruits)\\n莓果奇思 (Berry Concepts)\\n果乐园 (Fruit Haven)\\n水果之尚 (Fruit Couture)\\n果之宴 (Fruit Feast)\\n果之巧思 (Fruit Imagination)\\n果之臻 (Fruit Essence)\\n果之雅 (Fruit Elegance)\\n果之馨 (Fruit Fragrance)'\n"
     ]
    }
   ],
   "source": [
    "# 导入Langchain库中的OpenAI模块，该模块提供了与OpenAI语言模型交互的功能\n",
    "from langchain.llms import OpenAI  \n",
    "# 导入Langchain库中的PromptTemplate模块，用于创建和管理提示模板\n",
    "from langchain.prompts import PromptTemplate  \n",
    "# 导入Langchain库中的LLMChain模块，它允许构建基于大型语言模型的处理链\n",
    "from langchain.chains import LLMChain  \n",
    "# 导入dotenv库，用于从.env文件加载环境变量，这对于管理敏感数据如API密钥很有用\n",
    "from dotenv import load_dotenv  \n",
    "# 导入Langchain库中的ChatOpenAI类，用于创建和管理OpenAI聊天模型的实例。\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "#load_dotenv()  # 调用dotenv库的load_dotenv函数来加载.env文件中的环境变量。这通常用于管理敏感数据，如API密钥。\n",
    "\n",
    "# 创建一个ChatOpenAI实例，配置它使用gpt-3.5-turbo模型，\n",
    "# 设定温度参数为0.7（控制创造性的随机性）和最大令牌数为120（限制响应长度）。\n",
    "chat = ChatOpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=120\n",
    ")\n",
    "# 导入Langchain库中的模板类，用于创建聊天式的提示。\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "# 定义一个系统消息模板，用来设定AI的角色和任务（这里是起名字专家）。\n",
    "template = \"你是一位起名字专家，负责为专注于{product}的公司起名。\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "# 定义一个人类消息模板，用来模拟用户的提问（这里是请求为公司起名）。\n",
    "human_template = \"请为我们的公司起个名字，我们专注于{product_detail}。\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "# 将系统消息和人类消息的模板组合成一个聊天提示模板。\n",
    "prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# 使用聊天提示模板生成具体的聊天提示，这里指定产品为“水果”和产品细节为“高端送礼设计”。\n",
    "prompt = prompt_template.format_prompt(product=\"水果\", product_detail=\"高端送礼设计\").to_messages()\n",
    "\n",
    "# 使用chat函数（需要事先定义）发送生成的提示，获取结果。\n",
    "result = chat(prompt)\n",
    "\n",
    "# 打印聊天结果。\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输出parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确定需求：首先明确你需要模型输出什么样的格式，比如是JSON、CSV、数据库记录还是自定义的结构。\n",
    "\n",
    "查找插件：在 LangChain 提供的插件库中查找符合你需求的 OutputParser 插件。\n",
    "\n",
    "评估插件：评估每个 OutputParser 插件的功能、性能和易用性。考虑插件的文档是否齐全，社区支持情况，以及是否有足够的测试案例。\n",
    "\n",
    "测试插件：在实际的应用环境中测试选定的 OutputParser 插件，确保它能够正确地处理模型输出，并且满足你的需求。\n",
    "\n",
    "配置和使用：在 LangChain 框架中配置选定的 OutputParser 插件，并将其集成到你的应用流程中。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchcpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
